; Sample configuration for alternate models
;
; See also https://redink.ai/get-more for auto-installation files

; ================ GOOGLE MODELS ================

[Google Gemini 2.5 Pro minimum reasoning]

ModelNote = location: NL

APIKey = [[Your Google private_key]]
APIKeyEncrypted = False
OAuth2 = True
OAuth2ClientMail = [[Your Google client_email]]
OAuth2Scopes = https://www.googleapis.com/auth/cloud-platform
OAuth2Endpoint = https://oauth2.googleapis.com/token
OAuth2ATExpiry = 3600

Model = gemini-2.5-pro
Endpoint = https://europe-west4-aiplatform.googleapis.com/v1/projects/[[Your Google project_id]]/locations/europe-west4/publishers/google/models/{model}:generateContent
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"contents": [{"role": "user", "parts":[{"text": "{promptsystem} {promptuser}"}{objectcall}]}], "generationConfig": {"temperature": {temperature},  "thinking_config": {"thinking_budget": 128}}}
APICall_Object = , {"inlineData": {"mimeType": "{mimetype}","data": "{encodeddata}"}}
Response = text

Timeout = 200000
Temperature = 0.2

Updatesource = https://redink.ai/config/redink-config-model-google-gemini-2.5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[Google Gemini 2.5 Pro auto reasoning]

ModelNote = location: NL

APIKey = [[Your Google private_key]]
APIKeyEncrypted = False
OAuth2 = True
OAuth2ClientMail = [[Your Google client_email]]
OAuth2Scopes = https://www.googleapis.com/auth/cloud-platform
OAuth2Endpoint = https://oauth2.googleapis.com/token
OAuth2ATExpiry = 3600

Model = gemini-2.5-pro
Endpoint = https://europe-west4-aiplatform.googleapis.com/v1/projects/[[Your Google project_id]]/locations/europe-west4/publishers/google/models/{model}:generateContent
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"contents": [{"role": "user", "parts":[{"text": "{promptsystem} {promptuser}"}{objectcall}]}], "generationConfig": {"temperature": {temperature}}}
APICall_Object = , {"inlineData": {"mimeType": "{mimetype}","data": "{encodeddata}"}}
Response = text

Timeout = 200000
Temperature = 0.2

Updatesource = https://redink.ai/config/redink-config-model-google-gemini-2.5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[Google Gemini 2.5 Pro auto reasoning (T)]

ModelNote = location: NL

Model = gemini-2.5-pro

APIKey = [[Your Google private_key]]
APIKeyEncrypted = False
OAuth2 = True
OAuth2ClientMail = [[Your Google client_email]]
OAuth2Scopes = https://www.googleapis.com/auth/cloud-platform
OAuth2Endpoint = https://oauth2.googleapis.com/token
OAuth2ATExpiry = 3600

Endpoint = https://europe-west4-aiplatform.googleapis.com/v1/projects/[[Your Google project_id]]/locations/europe-west4/publishers/google/models/{model}:generateContent
HeaderA = Authorization
HeaderB = Bearer {apikey}

; changed for tooling

APICall = {"contents": [{"role": "user", "parts":[{"text": "{promptsystem} {promptuser}"}{objectcall}]}], "generationConfig": {"temperature": {temperature}}{toolinstructions}{toolresponses}}
APICall_Object = , {"inlineData": {"mimeType": "{mimetype}","data": "{encodeddata}"}}
Response = text (toolcall:<"functionCall">)

APICall_ToolInstructions = , "tools": [{"functionDeclarations": [{definitions}]}]
APICall_ToolInstructions_Template = {"name": "{name}", "description": "{description}", "parameters": {parameters}}
ToolCallExtractionMap = {"array_path":"$.candidates[0].content.parts[*].functionCall","call_id_path":"name","name_path":"name","arguments_path":"args"}
APICall_ToolResponses = , "contents": [{"role": "model", "parts": [{functioncalls}]}, {"role": "user", "parts": [{responses}]}]
APICall_ToolCallPart_Template = {"functionCall": {call}}
APICall_ToolResponses_Template = {"functionResponse": {"name": "{name}", "response": {response}}}

Timeout = 200000
Temperature = 0.2

[Google Gemini 2.5 Pro auto reasoning + Internet search]

ModelNote = location: NL

APIKey = [[Your Google private_key]]
APIKeyEncrypted = False
OAuth2 = True
OAuth2ClientMail = [[Your Google client_email]]
OAuth2Scopes = https://www.googleapis.com/auth/cloud-platform
OAuth2Endpoint = https://oauth2.googleapis.com/token
OAuth2ATExpiry = 3600

Model = gemini-2.5-pro
Endpoint = https://europe-west4-aiplatform.googleapis.com/v1/projects/[[Your Google project_id]]/locations/europe-west4/publishers/google/models/{model}:generateContent
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"contents": [{"role": "user", "parts":[{"text": "{promptsystem} {promptuser}"}{objectcall}]}], "generationConfig": {"temperature": {temperature}}, "tools": [{"google_search": {}}]}
APICall_Object = , {"inlineData": {"mimeType": "{mimetype}","data": "{encodeddata}"}}
Response = text

Timeout = 400000
Temperature = 0.2

Updatesource = https://redink.ai/config/redink-config-model-google-gemini-2.5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[Google Gemini 2.5 Pro maximum reasoning]

ModelNote = location: NL

APIKey = [[Your Google private_key]]
APIKeyEncrypted = False
OAuth2 = True
OAuth2ClientMail = [[Your Google client_email]]
OAuth2Scopes = https://www.googleapis.com/auth/cloud-platform
OAuth2Endpoint = https://oauth2.googleapis.com/token
OAuth2ATExpiry = 3600

Model = gemini-2.5-pro
Endpoint = https://europe-west4-aiplatform.googleapis.com/v1/projects/[[Your Google project_id]]/locations/europe-west4/publishers/google/models/{model}:generateContent
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"contents": [{"role": "user", "parts":[{"text": "{promptsystem} {promptuser}"}{objectcall}]}], "generationConfig": {"temperature": {temperature},  "thinking_config": {"thinking_budget": 32768}}}
APICall_Object = , {"inlineData": {"mimeType": "{mimetype}","data": "{encodeddata}"}}
Response = text

Timeout = 400000
Temperature = 0.2

Updatesource = https://redink.ai/config/redink-config-model-google-gemini-2.5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[Google Gemini 2.5 Flash no reasoning]

ModelNote = location: NL

APIKey = [[Your Google private_key]]
APIKeyEncrypted = False
OAuth2 = True
OAuth2ClientMail = [[Your Google client_email]]
OAuth2Scopes = https://www.googleapis.com/auth/cloud-platform
OAuth2Endpoint = https://oauth2.googleapis.com/token
OAuth2ATExpiry = 3600

Model = gemini-2.5-flash
Endpoint = https://europe-west4-aiplatform.googleapis.com/v1/projects/[[Your Google project_id]]/locations/europe-west4/publishers/google/models/{model}:generateContent
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"contents": [{"role": "user", "parts":[{"text": "{promptsystem} {promptuser}"}{objectcall}]}], "generationConfig": {"temperature": {temperature},  "thinking_config": {"thinking_budget": 0}}}
APICall_Object = , {"inlineData": {"mimeType": "{mimetype}","data": "{encodeddata}"}}
Response = text

Timeout = 200000
Temperature = 0.2

Updatesource = https://redink.ai/config/redink-config-model-google-gemini-2.5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[Google Gemini 2.5 Flash auto reasoning]

ModelNote = location: NL

APIKey = [[Your Google private_key]]
APIKeyEncrypted = False
OAuth2 = True
OAuth2ClientMail = [[Your Google client_email]]
OAuth2Scopes = https://www.googleapis.com/auth/cloud-platform
OAuth2Endpoint = https://oauth2.googleapis.com/token
OAuth2ATExpiry = 3600

Model = gemini-2.5-flash
Endpoint = https://europe-west4-aiplatform.googleapis.com/v1/projects/[[Your Google project_id]]/locations/europe-west4/publishers/google/models/{model}:generateContent
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"contents": [{"role": "user", "parts":[{"text": "{promptsystem} {promptuser}"}{objectcall}]}], "generationConfig": {"temperature": {temperature}}}
APICall_Object = , {"inlineData": {"mimeType": "{mimetype}","data": "{encodeddata}"}}
Response = text

Timeout = 200000
Temperature = 0.2

Updatesource = https://redink.ai/config/redink-config-model-google-gemini-2.5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[Google Gemini 2.5 Flash - minimal reasoning Internet search]

ModelNote = location: NL

APIKey = [[Your Google private_key]]
APIKeyEncrypted = False
OAuth2 = True
OAuth2ClientMail = [[Your Google client_email]]
OAuth2Scopes = https://www.googleapis.com/auth/cloud-platform
OAuth2Endpoint = https://oauth2.googleapis.com/token
OAuth2ATExpiry = 3600

Model = gemini-2.5-flash
Endpoint = https://europe-west4-aiplatform.googleapis.com/v1/projects/[[Your Google project_id]]/locations/europe-west4/publishers/google/models/{model}:generateContent
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"contents": [{"role": "user", "parts":[{"text": "{promptsystem} {promptuser}"}{objectcall}]}], "generationConfig": {"temperature": {temperature},  "thinking_config": {"thinking_budget": 256}}, "tools": [{"google_search": {}}]}
APICall_Object = , {"inlineData": {"mimeType": "{mimetype}","data": "{encodeddata}"}}
Response = text

Timeout = 100000
Temperature = 0.2

Updatesource = https://redink.ai/config/redink-config-model-google-gemini-2.5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[Google Gemini 2.5 Flash maximum reasoning + Internet search]

ModelNote = location: NL

APIKey = [[Your Google private_key]]
APIKeyEncrypted = False
OAuth2 = True
OAuth2ClientMail = [[Your Google client_email]]
OAuth2Scopes = https://www.googleapis.com/auth/cloud-platform
OAuth2Endpoint = https://oauth2.googleapis.com/token
OAuth2ATExpiry = 3600

Model = gemini-2.5-flash
Endpoint = https://europe-west4-aiplatform.googleapis.com/v1/projects/[[Your Google project_id]]/locations/europe-west4/publishers/google/models/{model}:generateContent
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"contents": [{"role": "user", "parts":[{"text": "{promptsystem} {promptuser}"}{objectcall}]}], "generationConfig": {"temperature": {temperature},  "thinking_config": {"thinking_budget": 24576}}, "tools": [{"google_search": {}}]}
APICall_Object = , {"inlineData": {"mimeType": "{mimetype}","data": "{encodeddata}"}}
Response = text

Timeout = 300000
Temperature = 0.2

Updatesource = https://redink.ai/config/redink-config-model-google-gemini-2.5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

; ================ OPENAI MODELS ================


; Legacy Note:
;
; If your version of Red Ink returns an error when trying to use "(file)" or "(clip)", then update your version (or use "Previe") or use one of 
; the following lines instead for the APICALL_Object parameter (the issue relates to OpenAI requiring separate call formats for separate file types)
;
; PDF OCR: 		APICall_Object = ,{"type": "input_file","filename": "userfile", "file_data": "data:{mimetype};base64,{encodeddata}"}
; Image recognition:    APICall_Object = ,{"type": "input_image","image_url": "data:{mimetype};base64,{encodeddata}"}

[OpenAI GPT-5.2 low reasoning]

ModelNote = location: global

APIKey = [[Your OpenAI API Key]]
APIKeyEncrypted = False
APIKeyPrefix = [[Your OpenAI API Key Prefix]]

Model = gpt-5.2
Endpoint = https://api.openai.com/v1/responses
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"model": "{model}", "input": [{"role": "developer", "content": [{"type": "input_text","text": "{promptsystem}"}]},{"role": "user","content": [{"type": "input_text","text": "{promptuser}"}{objectcall}]}],"reasoning": {"effort": "low"}}
APICall_Object = [application/pdf],{"type": "input_file","filename": "userfile.pdf", "file_data": "data:{mimetype};base64,{encodeddata}"}¦[image/png,image/jpeg,image/webp, image/gif],{"type": "input_image","image_url": "data:{mimetype};base64,{encodeddata}"}
Response = text (rkmode_first)

Temperature = 1.0
Timeout = 200000

Updatesource = https://redink.ai/config/redink-config-model-openai-gpt-5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[OpenAI GPT-5.2 auto reasoning]

ModelNote = location: global

APIKey = [[Your OpenAI API Key]]
APIKeyEncrypted = False
APIKeyPrefix = [[Your OpenAI API Key Prefix]]

Model = gpt-5.2
Endpoint = https://api.openai.com/v1/responses
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"model": "{model}", "input": [{"role": "developer", "content": [{"type": "input_text","text": "{promptsystem}"}]},{"role": "user","content": [{"type": "input_text","text": "{promptuser}"}{objectcall}]}]}
APICall_Object = [application/pdf],{"type": "input_file","filename": "userfile.pdf", "file_data": "data:{mimetype};base64,{encodeddata}"}¦[image/png,image/jpeg,image/webp, image/gif],{"type": "input_image","image_url": "data:{mimetype};base64,{encodeddata}"}
Response = text (rkmode_first)

Temperature = 1.0
Timeout = 400000

Updatesource = https://redink.ai/config/redink-config-model-openai-gpt-5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[OpenAI GPT-5.2 auto reasoning (T)]

ModelNote = location: global

APIKey = [[Your OpenAI API Key]]
APIKeyEncrypted = False
APIKeyPrefix = [[Your OpenAI API Key Prefix]]

Model = gpt-5.2
Endpoint = https://api.openai.com/v1/responses
HeaderA = Authorization
HeaderB = Bearer {apikey}

; changed for tooling

APICall = {"model": "{model}", "input": [{"role": "developer", "content": [{"type": "input_text","text": "{promptsystem}"}]},{"role": "user","content": [{"type": "input_text","text": "{promptuser}"}{objectcall}]}{toolresponses}]{toolinstructions}}
APICall_Object = [application/pdf],{"type": "input_file","filename": "userfile.pdf", "file_data": "data:{mimetype};base64,{encodeddata}"}¦[image/png,image/jpeg,image/webp, image/gif],{"type": "input_image","image_url": "data:{mimetype};base64,{encodeddata}"}
Response = text (rkmode_first) (toolcall:<"type"\s*:\s*"function_call">)

APICall_ToolInstructions = , "tools": [{definitions}]
APICall_ToolInstructions_Template = {"type": "function", "name": "{name}", "description": "{description}", "parameters": {parameters}}
ToolCallExtractionMap = {"array_path":"$.output[?(@.type=='function_call')]","call_id_path":"call_id","name_path":"name","arguments_path":"arguments"}
APICall_ToolResponses = {functioncalls}{responses}
APICall_ToolCallPart_Template = ,{"type": "function_call", "call_id": "{call_id}", "name": "{name}", "arguments": "{arguments}"}
APICall_ToolResponses_Template = ,{"type": "function_call_output", "call_id": "{call_id}", "output": "{response}"}

Temperature = 1.0
Timeout = 400000

Updatesource = https://redink.ai/config/redink-config-model-openai-gpt-5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[OpenAI GPT-5.2 medium reasoning]

ModelNote = location: global

APIKey = [[Your OpenAI API Key]]
APIKeyEncrypted = False
APIKeyPrefix = [[Your OpenAI API Key Prefix]]

Model = gpt-5.2
Endpoint = https://api.openai.com/v1/responses
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"model": "{model}", "input": [{"role": "developer", "content": [{"type": "input_text","text": "{promptsystem}"}]},{"role": "user","content": [{"type": "input_text","text": "{promptuser}"}{objectcall}]}],"reasoning": {"effort": "medium"}}
APICall_Object = [application/pdf],{"type": "input_file","filename": "userfile.pdf", "file_data": "data:{mimetype};base64,{encodeddata}"}¦[image/png,image/jpeg,image/webp, image/gif],{"type": "input_image","image_url": "data:{mimetype};base64,{encodeddata}"}
Response = text (rkmode_first)

Temperature = 1.0
Timeout = 400000

Updatesource = https://redink.ai/config/redink-config-model-openai-gpt-5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[OpenAI GPT-5.2 high reasoning]

ModelNote = location: global

APIKey = [[Your OpenAI API Key]]
APIKeyEncrypted = False
APIKeyPrefix = [[Your OpenAI API Key Prefix]]

Model = gpt-5.2
Endpoint = https://api.openai.com/v1/responses
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"model": "{model}", "input": [{"role": "developer", "content": [{"type": "input_text","text": "{promptsystem}"}]},{"role": "user","content": [{"type": "input_text","text": "{promptuser}"}{objectcall}]}],"reasoning": {"effort": "high"}}
APICall_Object = [application/pdf],{"type": "input_file","filename": "userfile.pdf", "file_data": "data:{mimetype};base64,{encodeddata}"}¦[image/png,image/jpeg,image/webp, image/gif],{"type": "input_image","image_url": "data:{mimetype};base64,{encodeddata}"}
Response = text (rkmode_first)

Temperature = 1.0
Timeout = 600000

Updatesource = https://redink.ai/config/redink-config-model-openai-gpt-5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[OpenAI GPT-5.2 low reasoning + Internet search]

ModelNote = location: global

APIKey = [[Your OpenAI API Key]]
APIKeyEncrypted = False
APIKeyPrefix = [[Your OpenAI API Key Prefix]]

Model = gpt-5.2
Endpoint = https://api.openai.com/v1/responses
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"model": "{model}", "tools": [{"type": "web_search"}], "input": [{"role": "developer", "content": [{"type": "input_text","text": "{promptsystem}"}]},{"role": "user","content": [{"type": "input_text","text": "{promptuser}"}{objectcall}]}],"reasoning": {"effort": "low"}}
APICall_Object = [application/pdf],{"type": "input_file","filename": "userfile.pdf", "file_data": "data:{mimetype};base64,{encodeddata}"}¦[image/png,image/jpeg,image/webp, image/gif],{"type": "input_image","image_url": "data:{mimetype};base64,{encodeddata}"}
Response = text (rkmode_first)

Temperature = 1.0
Timeout = 400000

Updatesource = https://redink.ai/config/redink-config-model-openai-gpt-5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[OpenAI GPT-5.2 auto reasoning + Internet search]

ModelNote = location: global

APIKey = [[Your OpenAI API Key]]
APIKeyEncrypted = False
APIKeyPrefix = [[Your OpenAI API Key Prefix]]

Model = gpt-5.2
Endpoint = https://api.openai.com/v1/responses
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"model": "{model}", "tools": [{"type": "web_search"}], "input": [{"role": "developer", "content": [{"type": "input_text","text": "{promptsystem}"}]},{"role": "user","content": [{"type": "input_text","text": "{promptuser}"}{objectcall}]}]}
APICall_Object = [application/pdf],{"type": "input_file","filename": "userfile.pdf", "file_data": "data:{mimetype};base64,{encodeddata}"}¦[image/png,image/jpeg,image/webp, image/gif],{"type": "input_image","image_url": "data:{mimetype};base64,{encodeddata}"}
Response = text (rkmode_first)

Temperature = 1.0
Timeout = 600000

Updatesource = https://redink.ai/config/redink-config-model-openai-gpt-5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[OpenAI GPT-5-mini auto reasoning]

ModelNote = location: global

APIKey = [[Your OpenAI API Key]]
APIKeyEncrypted = False
APIKeyPrefix = [[Your OpenAI API Key Prefix]]

Model = gpt-5-mini
Endpoint = https://api.openai.com/v1/responses
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"model": "{model}", "input": [{"role": "developer", "content": [{"type": "input_text","text": "{promptsystem}"}]},{"role": "user","content": [{"type": "input_text","text": "{promptuser}"}{objectcall}]}]}
APICall_Object = [application/pdf],{"type": "input_file","filename": "userfile.pdf", "file_data": "data:{mimetype};base64,{encodeddata}"}¦[image/png,image/jpeg,image/webp, image/gif],{"type": "input_image","image_url": "data:{mimetype};base64,{encodeddata}"}
Response = text (rkmode_first)

Temperature = 1.0
Timeout = 200000

Updatesource = https://redink.ai/config/redink-config-model-openai-gpt-5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[OpenAI GPT-5-mini auto reasoning + Internet search]

ModelNote = location: global

APIKey = [[Your OpenAI API Key]]
APIKeyEncrypted = False
APIKeyPrefix = [[Your OpenAI API Key Prefix]]

Model = gpt-5-mini
Endpoint = https://api.openai.com/v1/responses
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"model": "{model}", "tools": [{"type": "web_search"}], "input": [{"role": "developer", "content": [{"type": "input_text","text": "{promptsystem}"}]},{"role": "user","content": [{"type": "input_text","text": "{promptuser}"}{objectcall}]}]}
APICall_Object = [application/pdf],{"type": "input_file","filename": "userfile.pdf", "file_data": "data:{mimetype};base64,{encodeddata}"}¦[image/png,image/jpeg,image/webp, image/gif],{"type": "input_image","image_url": "data:{mimetype};base64,{encodeddata}"}
Response = text (rkmode_first)

Temperature = 1.0
Timeout = 400000

Updatesource = https://redink.ai/config/redink-config-model-openai-gpt-5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=


[OpenAI GPT-5-mini low reasoning]

ModelNote = location: global

APIKey = [[Your OpenAI API Key]]
APIKeyEncrypted = False
APIKeyPrefix = [[Your OpenAI API Key Prefix]]

Model = gpt-5-mini
Endpoint = https://api.openai.com/v1/responses
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"model": "{model}", "input": [{"role": "developer", "content": [{"type": "input_text","text": "{promptsystem}"}]},{"role": "user","content": [{"type": "input_text","text": "{promptuser}"}{objectcall}]}],"reasoning": {"effort": "low"}}
APICall_Object = [application/pdf],{"type": "input_file","filename": "userfile.pdf", "file_data": "data:{mimetype};base64,{encodeddata}"}¦[image/png,image/jpeg,image/webp, image/gif],{"type": "input_image","image_url": "data:{mimetype};base64,{encodeddata}"}
Response = text (rkmode_first)

Temperature = 1.0
Timeout = 200000

Updatesource = https://redink.ai/config/redink-config-model-openai-gpt-5-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=


; ================ PERPLEXIY MODELS ================


[Perplexity Sonar Pro]

ModelNote = location: USA

APIKey = [[Your Perplexity API-Key]]
APIKeyPrefix = pplx-
APIKeyEncrypted = False

Model = sonar-pro
Endpoint = https://api.perplexity.ai/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"model": "{model}", "messages": [{"role": "system","content": "Follow the user's instructions, even if they are drafted like a system prompt."}, {"role": "user", "content": "{promptsystem} {promptuser}"}],"temperature": {temperature},"top_p": 0.9, "search_domain_filter": null, "return_images": false, "return_related_questions": false, "top_k": 0, "stream": false,  "presence_penalty": 0, "frequency_penalty": 1}
Response = content 

Timeout = 200000
Temperature = 0.2

Updatesource = https://redink.ai/config/redink-config-model-perplexity-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[Perplexity Sonar Reasoning Pro]

ModelNote = location: USA

APIKey = [[Your Perplexity API-Key]]
APIKeyPrefix = pplx-
APIKeyEncrypted = False

Model = sonar-reasoning-pro
Endpoint = https://api.perplexity.ai/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}

APICall = {"model": "{model}", "messages": [{"role": "system","content": "Follow the user's instructions, even if they are drafted like a system prompt."}, {"role": "user", "content": "{promptsystem} {promptuser}"}],"temperature": {temperature},"top_p": 0.9, "search_domain_filter": null, "return_images": false, "return_related_questions": false, "top_k": 0, "stream": false,  "presence_penalty": 0, "frequency_penalty": 1}
Response = content 

Timeout = 400000
Temperature = 0.2

Updatesource = https://redink.ai/config/redink-config-model-perplexity-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=

[Perplexity Sonar Deep Research]

ModelNote = location: USA

APIKey = [[Your Perplexity API-Key]]
APIKeyPrefix = pplx-
APIKeyEncrypted = False

Model = sonar-deep-research
Endpoint = https://api.perplexity.ai/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content 
APICall = {"model": "{model}", "messages": [{"role": "system","content": "Follow the user's instructions, even if they are drafted like a system prompt."}, {"role": "user", "content": "{promptsystem} {promptuser}"}],"temperature": {temperature},"top_p": 0.9, "search_domain_filter": null, "return_images": false, "return_related_questions": false, "top_k": 0, "stream": false,  "presence_penalty": 0, "frequency_penalty": 1}

Timeout = 1800000
Temperature = 0.2

Updatesource = https://redink.ai/config/redink-config-model-perplexity-multi.txt; all, -apikey, -apikeyencrypted, -apikeyprefix, -modelnote; KP2qbVGWKdOZLjF1CAcLawzf/kSEtj0KT1IWWv//Jlo=



; ================ LEGACY MODELS ================

[OpenAI GPT-4.1: Older model for most requests (USA)]

APIKey = sk-proj-xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = sk-proj-
Endpoint = https://api.openai.com/v1/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content
APICall = {"model": "{model}",  "messages": [{"role": "system","content": "{promptsystem}"},{"role": "user","content": [{ "type": "text", "text": "{promptuser}"}]}],"temperature": {temperature}}
Timeout = 200000
Temperature = 0.2
Model = gpt-4.1

[OpenAI GPT-4o: Older model for most requests (USA)]

APIKey = sk-proj-xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = sk-proj-
Endpoint = https://api.openai.com/v1/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content
APICall = {"model": "{model}",  "messages": [{"role": "system","content": "{promptsystem}"},{"role": "user","content": [{ "type": "text", "text": "{promptuser}"}]}],"temperature": {temperature}}
Timeout = 200000
Temperature = 0.2
Model = gpt-4o

[OpenAI GPT-4o-search-preview: Older model plus Internet search (USA)]

APIKey = sk-proj-xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = sk-proj-
Endpoint = https://api.openai.com/v1/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content
APICall = {"model": "{model}",  "web_search_options": {}, "messages": [{"role": "system","content": "{promptsystem}"},{"role": "user","content": [{ "type": "text", "text": "{promptuser}"}{objectcall}]}]}
APICall_Object = , {"type": "image_url", "image_url": {"url": "data:{mimetype};base64,{encodeddata}"}}
Timeout = 200000
Temperature = 0.2
Model = gpt-4o-search-preview


[OpenAI o3-mini: Older, fast reasoning model (5 Min. Timeout, USA)]

APIKey = sk-proj-xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = sk-proj-
Endpoint = https://api.openai.com/v1/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content
APICall = {"model": "{model}",  "messages": [{"role": "user","content": "{promptsystem} {promptuser}"}],"temperature": {temperature}}
Timeout = 300000
Temperature = 1.0
Model = o3-mini

[OpenAI o3: Older, more advanced reasoning model (5 Min. Timeout, USA)]

APIKey = sk-proj-xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = sk-proj-
Endpoint = https://api.openai.com/v1/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content
APICall = {"model": "{model}",  "messages": [{"role": "user","content": "{promptsystem} {promptuser}"}],"temperature": {temperature}}	
Timeout = 300000
Temperature = 1.0
Model = o3

[OpenAI o1: High-end reasoning model ($, 10 Min. Timeout, USA)]

APIKey = sk-proj-xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = sk-proj-
Endpoint = https://api.openai.com/v1/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content
APICall = {"model": "{model}",  "messages": [{"role": "user","content": "{promptsystem} {promptuser}"}],"temperature": {temperature}}
Timeout = 600000
Temperature = 1.0
Model = o1

[OpenAI o4 Mini Deep Research: Normal Deep Research (10 Min. Timeout, USA)]

APIKey = sk-proj-xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = sk-proj-
Endpoint = https://api.openai.com/v1/responses
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = text
APICall = {"model": "{model}", "input": "{promptsystem} {promptuser}", "tools": [{ "type": "web_search_preview" },{ "type": "code_interpreter", "container": { "type": "auto" } }]}
Timeout = 600000
Temperature = 1.0
Model = o4-mini-deep-research

[OpenAI o3 Deep Research: Higher-End Deep Research ($$, 20 Min. Timeout, USA)]

APIKey = sk-proj-xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = sk-proj-
Endpoint = https://api.openai.com/v1/responses
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = text
APICall = {"model": "{model}", "input": "{promptsystem} {promptuser}", "tools": [{ "type": "web_search_preview" },{ "type": "code_interpreter", "container": { "type": "auto" } }]}
Timeout = 1200000
Temperature = 1.0
Model = o3-deep-research

[OpenAI GPT Image-1: Image Creation (USA)]

APIKey = sk-proj-xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = sk-proj-
Endpoint = https://api.openai.com/v1/images/generations
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = text
APICall = {"model": "{model}", "prompt": "{promptsystem} {promptuser}"}
Timeout = 200000
Temperature = 1.0
Model = gpt-image-1

[OpenAI GPT Image-1: Image Editing (USA)]

APIKey = sk-proj-xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = sk-proj-
Endpoint = https://api.openai.com/v1/images/edits
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = text
APICall = not used
APICall_Object = multipart:model:{model};prompt:{promptsystem} {promptuser};filefield:image[]
Timeout = 500000
Temperature = 1.0
Model = gpt-image-1

[Infomaniak: Qwen3, no reasoning (CH)]

; Qwen3-235B-A22B-Instruct-2507

APIKey = xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = 
Endpoint = https://api.infomaniak.com/1/ai/[infomaniak_product_id]/openai/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content
APICall = {"model": "{model}",  "messages": [{"role": "user","content": "{promptsystem} {promptuser}"}],"temperature": {temperature}}
Timeout = 200000
Temperature = 0.2
Model = qwen3

[Infomaniak: DeepSeek-R1, reasoning (CH)]

; DeepSeek-R1-distilled-qwen-32B

APIKey = xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = 
Endpoint = https://api.infomaniak.com/1/ai/[infomaniak_product_id]/openai/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content (nothink)
APICall = {"model": "{model}",  "messages": [{"role": "user","content": "{promptsystem} {promptuser}"}],"temperature": {temperature}}
Timeout = 300000
Temperature = 0.2
Model = reasoning

[Infomaniak: Flux, image generation (CH, use 'Pure:' prefix)]

; Use "Pure:" prefix in Freestyle

APIKey = xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = 
Endpoint = https://api.infomaniak.com/1/ai/[infomaniak_product_id]/openai/images/generations
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = revised_prompt
APICall = {"model": "{model}",  "prompt": "{promptsystem} {promptuser}", "quality": "standard"}
;APICall = {"model": "{model}",  "prompt": "{promptsystem} {promptuser}", "quality": "hd", "size": "1792x1024"}
Timeout = 400000
Temperature = 1
Model = flux

[MTF: Llama3, no reasoning (CH)]

APIKey = xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = 
Endpoint = https://api.ai.mtf.cloud/chatbot/ask
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content
APICall = {"model": "{model}",  "messages": [{"role": "user","content": "{promptsystem} {promptuser}"}],"temperature": {temperature}}
Timeout = 200000
Temperature = 0.2
Model = meta-llama-ai


[SafeSwissCloud: gpt-oss-120b (CH)]

APIKey = xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = 
Endpoint = https://llm01.safeswisscloud.ch/engines/{model}/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content
APICall = {"model": "{model}",  "messages": [{"role": "user","content": "{promptsystem} {promptuser}"}],"temperature": {temperature}}
Timeout = 200000
Temperature = 0.2
Model = gpt-oss-120b


[SafeSwissCloud: Variable Model (CH)]

APIKey = xxxxxx
APIKeyEncrypted = False
APIKeyPrefix = 
Endpoint = https://llm01.safeswisscloud.ch/engines/{model}/chat/completions
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content
APICall = {"model": "{model}",  "messages": [{"role": "user","content": "{promptsystem} {promptuser}"}],"temperature": {temperature}}
Timeout = 200000
Temperature = 0.2
Model = {parameter1=Modell; String; gpt-oss-120b; apertus-8b, apertus-70b, deepseekr1-70b, deepseekr1-670b, mistral-v03-7b, qwen3-8b, qwq-32b, qwq25-vl-72b, llama33-70b, llama4-maverick, llama4-scout-17b, granite-33-8b, granite-emb-278m, bge-m3, kimi-k2, gemma-12b-it, granite-vision-2b, gpt-oss-120b}



[Ollama: qwen3:30b]

; Generic setup sample for LLM instances using Ollama, eg. running on a Portainer setup
; Contributed by layer-8-law

APIKey = xxxxxx
Endpoint = http://[DOMAIN-OR-IP-ADDRESS]:[PORT]/api/chat
; Endpoint: must point to your Ollama instance either using domain or IP and specific port (default:11434)
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content
APICall = {"model":"{model}","messages":[   {"role":"system","content":"{promptsystem}"},   {"role":"user","content":"{promptuser}"{objectcall}} ], "stream": false, "think": false, "options":{"temperature": {temperature}}}
; APICall: if you remove "think": false or set it to true, Ollama will respond including the thinking process. This will clutter up your documents or dialogs but may include useful hints for debugging.
Timeout = 100000
Temperature = 0.2
Model = qwen3:30b
; Model: set to your desired model as labeled in OpenWebUI or Ollama, eg. qwen3:30b or ministral-3:14b etc.
OAuth2 = False
OAuth2ClientMail = 
OAuth2Scopes = 
OAuth2Endpoint = 
OAuth2ATExpiry = 0

[OpenWebUI: qwen3:30b]

; Generic setup for LLM instances using OpenWebUI, eg. running on a Portainer setup]
; Contributed by layer-8-law

APIKey = xxxxxx
Endpoint = http://[DOMAIN-OR-IP-ADDRESS]:[PORT]/api/chat/completions
; Endpoint: must point to your OpenWebUI instance either using domain or IP and specific port (default: 3000)
HeaderA = Authorization
HeaderB = Bearer {apikey}
Response = content
APICall = {"model":"{model}","messages": [{"role":"system","content":"{promptsystem}"}, {"role":"user","content":"{promptuser}"{objectcall}}],"stream":false,"temperature":{temperature}}
; APICall: No "think": false or similar needed. OpenWebUI will respond only with content in this configuration.
Timeout = 100000
Temperature = 0.2
Model = qwen3:30b
; Model: set to your desired model as labeled in OpenWebUI or Ollama, eg. qwen3:30b or ministral-3:14b etc.
OAuth2 = False
OAuth2ClientMail = 
OAuth2Scopes = 
OAuth2Endpoint = 
OAuth2ATExpiry = 0